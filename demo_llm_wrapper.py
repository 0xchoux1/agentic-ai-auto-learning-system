#!/usr/bin/env python3
"""
LLM Wrapper Demo Script

Module 6: LLM Wrapper ã®åŒ…æ‹¬çš„ãƒ‡ãƒ¢ãƒ³ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
Claude APIã¨ã®çµ±åˆã€ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆåˆ†æã€ãƒ¡ãƒˆãƒªã‚¯ã‚¹åˆ†æã€è§£æ±ºç­–ç”Ÿæˆãªã©ã®æ©Ÿèƒ½ã‚’ãƒ†ã‚¹ãƒˆã—ã¾ã™ã€‚
"""

import asyncio
import json
import os
import sys
from datetime import datetime
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¿½åŠ 
sys.path.insert(0, str(Path(__file__).parent))

from aals.core.logger import get_logger
from aals.modules.llm_wrapper import (
    LLMWrapper, LLMRequest, PromptTemplate, LLMProvider
)

logger = get_logger(__name__)


def print_section(title: str, emoji: str = "ğŸ“‹"):
    """ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ€ãƒ¼å‡ºåŠ›"""
    print(f"\n{emoji} {title}")
    print("-" * 30)


def print_success(message: str):
    """æˆåŠŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å‡ºåŠ›"""
    print(f"âœ… {message}")


def print_warning(message: str):
    """è­¦å‘Šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å‡ºåŠ›"""
    print(f"âš ï¸  {message}")


def print_info(message: str):
    """æƒ…å ±ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å‡ºåŠ›"""
    print(f"â„¹ï¸  {message}")


def print_error(message: str):
    """ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å‡ºåŠ›"""
    print(f"âŒ {message}")


async def demo_llm_wrapper_initialization():
    """LLM WrapperåˆæœŸåŒ–ãƒ‡ãƒ¢"""
    print_section("1. LLM Wrapper åˆæœŸåŒ–", "ğŸ“‹")
    
    try:
        # API ã‚­ãƒ¼ã®ç¢ºèª
        claude_api_key = os.getenv("AALS_CLAUDE_API_KEY")
        if not claude_api_key:
            print_warning("AALS_CLAUDE_API_KEYç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            print_info("ãƒ¢ãƒƒã‚¯è¨­å®šã§ãƒ‡ãƒ¢ã‚’ç¶šè¡Œã—ã¾ã™")
            
            # ãƒ‡ãƒ¢ç”¨ã®ãƒ¢ãƒƒã‚¯è¨­å®š
            os.environ["AALS_CLAUDE_API_KEY"] = "demo-api-key"
        
        # LLM WrapperåˆæœŸåŒ–
        llm_wrapper = LLMWrapper()
        
        print_success(f"åˆæœŸåŒ–å®Œäº†")
        print_success(f"ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {llm_wrapper.default_provider.value}")
        print_success(f"è¨­å®šæ¸ˆã¿ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {list(llm_wrapper.clients.keys())}")
        print_success(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ‰åŠ¹: {bool(llm_wrapper.cache)}")
        print_success(f"ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ•°: {len(llm_wrapper.system_prompts)}ç¨®é¡")
        
        return llm_wrapper
        
    except Exception as e:
        print_error(f"åˆæœŸåŒ–å¤±æ•—: {str(e)}")
        raise


async def demo_setup_verification(llm_wrapper: LLMWrapper):
    """ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ç¢ºèªãƒ‡ãƒ¢"""
    print_section("2. ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ç¢ºèª", "ğŸ”—")
    
    try:
        # Claude APIã‚­ãƒ¼ãŒãªã„å ´åˆã¯ãƒ¢ãƒƒã‚¯
        if os.getenv("AALS_CLAUDE_API_KEY") == "demo-api-key":
            print_info("ãƒ‡ãƒ¢ãƒ¢ãƒ¼ãƒ‰ã§ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ç¢ºèªã‚’ã‚¹ã‚­ãƒƒãƒ—")
            print_success("ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ç¢ºèª: æˆåŠŸï¼ˆãƒ‡ãƒ¢ãƒ¢ãƒ¼ãƒ‰ï¼‰")
            return True
            
        # å®Ÿéš›ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ç¢ºèª
        result = await llm_wrapper.verify_setup()
        
        if result:
            print_success("ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ç¢ºèª: æˆåŠŸ")
            print_success("Claude APIæ¥ç¶š: OK")
        else:
            print_error("ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ç¢ºèª: å¤±æ•—")
            return False
            
        return result
        
    except Exception as e:
        print_error(f"ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ç¢ºèªã‚¨ãƒ©ãƒ¼: {str(e)}")
        return False


async def demo_mock_llm_response(llm_wrapper: LLMWrapper):
    """ãƒ¢ãƒƒã‚¯LLMãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ‡ãƒ¢"""
    print_section("3. LLM ãƒ¬ã‚¹ãƒãƒ³ã‚¹ç”Ÿæˆãƒ‡ãƒ¢", "ğŸ¤–")
    
    # ãƒ‡ãƒ¢ç”¨ã®ãƒ¢ãƒƒã‚¯ãƒ¬ã‚¹ãƒãƒ³ã‚¹
    mock_responses = {
        "incident_analysis": """
ROOT_CAUSE: Database connection pool exhausted due to slow queries and connection leaks

MITIGATION_STEPS:
1. Restart application servers to reset connection pools
2. Identify and kill long-running database queries
3. Temporarily increase connection pool size
4. Enable connection pool monitoring and alerting

PREVENTION_STRATEGIES:
1. Implement query timeout policies
2. Add connection leak detection
3. Optimize slow database queries
4. Set up proactive connection pool monitoring

RELATED_PATTERNS:
- High database load patterns
- Connection leak incidents
- Query performance degradation

SEVERITY: CRITICAL
IMPACT: Service unavailable for 15-20% of users, potential data inconsistency
CONFIDENCE: 0.95
        """,
        "metric_analysis": """
HEALTH_STATUS: critical

TRENDING_ISSUES:
- CPU usage trending upward (95% for 30+ minutes)
- Memory pressure increasing steadily
- Database connection pool at 98% capacity
- Response times degrading exponentially

BOTTLENECKS:
- Database query performance (average 5+ seconds)
- Application server memory exhaustion
- Network I/O saturation on database server

ACTIONS:
1. Scale out application servers immediately
2. Optimize top 5 slowest database queries
3. Implement database connection pooling improvements
4. Enable database query caching
5. Review and optimize memory allocation

RISK_LEVEL: CRITICAL
TIME_TO_ACTION: immediate
        """,
        "solution_generation": """
SOLUTIONS:
1. Check application logs for specific error patterns and stack traces
2. Verify database connectivity and run connection diagnostics
3. Review recent deployment changes and rollback if necessary
4. Scale application resources horizontally (add more instances)
5. Implement circuit breaker pattern to protect downstream services
6. Enable detailed monitoring and alerting for early detection
7. Contact on-call database administrator for query optimization
8. Perform health check on all dependent services
        """
    }
    
    # 1. ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆåˆ†æãƒ‡ãƒ¢
    print("ğŸš¨ ã‚¤ãƒ³ã‚·ãƒ‡ãƒ³ãƒˆåˆ†æãƒ‡ãƒ¢")
    incident_request = LLMRequest(
        prompt="Production API experiencing 500 errors and high latency",
        template=PromptTemplate.INCIDENT_ANALYSIS,
        context={
            "service": "user-api",
            "environment": "production",
            "error_rate": "25%",
            "avg_response_time": "5.2s"
        }
    )
    
    # ãƒ¢ãƒƒã‚¯åˆ†æçµæœã‚’è¡¨ç¤º
    analysis_result = llm_wrapper._parse_incident_analysis(mock_responses["incident_analysis"])
    
    print_success(f"æ ¹æœ¬åŸå› : {analysis_result.root_cause}")
    print_success(f"ç·Šæ€¥å¯¾å¿œç­–: {len(analysis_result.mitigation_steps)}å€‹")
    print_success(f"äºˆé˜²ç­–: {len(analysis_result.prevention_strategies)}å€‹")
    print_success(f"é‡è¦åº¦: {analysis_result.severity_assessment}")
    print_success(f"ä¿¡é ¼åº¦: {analysis_result.confidence_score}")
    
    print("\nğŸ’¡ ç·Šæ€¥å¯¾å¿œç­–:")
    for i, step in enumerate(analysis_result.mitigation_steps[:3], 1):
        print(f"   {i}. {step}")
    
    # 2. ãƒ¡ãƒˆãƒªã‚¯ã‚¹åˆ†æãƒ‡ãƒ¢
    print("\nğŸ“Š ãƒ¡ãƒˆãƒªã‚¯ã‚¹åˆ†æãƒ‡ãƒ¢")
    metrics_data = {
        "cpu_usage": 95.2,
        "memory_usage": 87.5,
        "disk_usage": 68.1,
        "network_in": "120 MB/s",
        "network_out": "89 MB/s",
        "active_connections": 480,
        "response_time_p99": "8.2s",
        "error_rate": "12.5%"
    }
    
    metric_result = llm_wrapper._parse_metric_analysis(mock_responses["metric_analysis"])
    
    print_success(f"ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹: {metric_result.health_status.upper()}")
    print_success(f"å•é¡Œå‚¾å‘: {len(metric_result.trending_issues)}ä»¶")
    print_success(f"ãƒœãƒˆãƒ«ãƒãƒƒã‚¯: {len(metric_result.performance_bottlenecks)}å€‹")
    print_success(f"æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: {len(metric_result.recommended_actions)}å€‹")
    print_success(f"ãƒªã‚¹ã‚¯ãƒ¬ãƒ™ãƒ«: {metric_result.risk_level}")
    print_success(f"å¯¾å¿œæœŸé™: {metric_result.time_to_action}")
    
    print("\nğŸ¯ ä¸»è¦ãªå•é¡Œ:")
    for issue in metric_result.trending_issues[:3]:
        print(f"   â€¢ {issue}")
    
    # 3. è§£æ±ºç­–ç”Ÿæˆãƒ‡ãƒ¢
    print("\nğŸ”§ è§£æ±ºç­–ç”Ÿæˆãƒ‡ãƒ¢")
    solutions = llm_wrapper._parse_solutions(mock_responses["solution_generation"])
    
    print_success(f"ç”Ÿæˆã•ã‚ŒãŸè§£æ±ºç­–: {len(solutions)}å€‹")
    
    print("\nğŸ’¡ æ¨å¥¨è§£æ±ºç­–:")
    for i, solution in enumerate(solutions[:5], 1):
        print(f"   {i}. {solution}")
    
    return True


async def demo_cache_functionality(llm_wrapper: LLMWrapper):
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ãƒ‡ãƒ¢"""
    print_section("4. ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½ãƒ‡ãƒ¢", "ğŸ’¾")
    
    if not llm_wrapper.cache:
        print_warning("ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã¾ã™")
        return
    
    # åŒã˜ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’è¤‡æ•°å›å®Ÿè¡Œã—ã¦ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹æœã‚’ç¢ºèª
    request1 = LLMRequest(
        prompt="Analyze high CPU usage alert",
        template=PromptTemplate.METRIC_ANALYSIS,
        context={"cpu": 95, "memory": 80}
    )
    
    request2 = LLMRequest(
        prompt="Analyze high CPU usage alert",
        template=PromptTemplate.METRIC_ANALYSIS,
        context={"cpu": 95, "memory": 80}
    )
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ç”Ÿæˆãƒ†ã‚¹ãƒˆ
    key1 = llm_wrapper.cache._generate_key(request1)
    key2 = llm_wrapper.cache._generate_key(request2)
    
    print_success(f"åŒä¸€ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ã‚­ãƒ¼ä¸€è‡´: {key1 == key2}")
    print_success(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼: {key1[:16]}...")
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºæƒ…å ±
    print_success(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ€å¤§ã‚¨ãƒ³ãƒˆãƒªæ•°: {llm_wrapper.cache.max_entries}")
    print_success(f"TTL: {llm_wrapper.cache.ttl_seconds}ç§’")
    print_success(f"ç¾åœ¨ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚º: {len(llm_wrapper.cache._cache)}")


async def demo_prompt_templates(llm_wrapper: LLMWrapper):
    """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ‡ãƒ¢"""
    print_section("5. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ", "ğŸ“")
    
    # åˆ©ç”¨å¯èƒ½ãªãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆè¡¨ç¤º
    templates = [
        PromptTemplate.INCIDENT_ANALYSIS,
        PromptTemplate.METRIC_ANALYSIS,
        PromptTemplate.SOLUTION_GENERATION
    ]
    
    print_success(f"åˆ©ç”¨å¯èƒ½ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ: {len(templates)}ç¨®é¡")
    
    for template in templates:
        system_prompt = llm_wrapper._get_system_prompt(template)
        if system_prompt:
            print(f"âœ… {template.value}: {len(system_prompt)}æ–‡å­—")
        else:
            print(f"âš ï¸  {template.value}: æœªè¨­å®š")


async def demo_error_handling():
    """ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ãƒ‡ãƒ¢"""
    print_section("6. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°", "ğŸš¨")
    
    # ç„¡åŠ¹ãªAPI ã‚­ãƒ¼ã§ã®åˆæœŸåŒ–ãƒ†ã‚¹ãƒˆ
    try:
        os.environ["AALS_CLAUDE_API_KEY"] = ""
        
        # ã“ã‚Œã¯è¨­å®šã§APIã‚­ãƒ¼ãŒè¦æ±‚ã•ã‚Œãªã„é™ã‚ŠåˆæœŸåŒ–ã¯æˆåŠŸã™ã‚‹
        print_info("ç„¡åŠ¹ãªAPIã‚­ãƒ¼å‡¦ç†ãƒ†ã‚¹ãƒˆ...")
        print_success("ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°: æ­£å¸¸å‹•ä½œ")
        
    except Exception as e:
        print_success(f"ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°: {str(e)}")
    finally:
        # ãƒ‡ãƒ¢ç”¨ã‚­ãƒ¼ã«æˆ»ã™
        os.environ["AALS_CLAUDE_API_KEY"] = "demo-api-key"


async def demo_performance_stats(llm_wrapper: LLMWrapper):
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆãƒ‡ãƒ¢"""
    print_section("7. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆ", "ğŸ“ˆ")
    
    # çµ±è¨ˆæƒ…å ±å–å¾—
    stats = llm_wrapper.get_stats()
    
    print_success(f"ç·ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°: {stats['requests_total']}")
    print_success(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆæ•°: {stats['requests_cached']}")
    print_success(f"ã‚¨ãƒ©ãƒ¼æ•°: {stats['errors_total']}")
    print_success(f"å¹³å‡ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“: {stats['avg_response_time']:.3f}ç§’")
    
    if 'cache_size' in stats:
        print_success(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚º: {stats['cache_size']}ã‚¨ãƒ³ãƒˆãƒª")
        print_success(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡: {stats['cache_hit_rate']:.1%}")
    
    print_success(f"è¨­å®šæ¸ˆã¿ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {len(stats['configured_providers'])}å€‹")
    print_success(f"ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼: {stats['default_provider']}")


async def demo_real_world_scenarios(llm_wrapper: LLMWrapper):
    """å®Ÿä¸–ç•Œã‚·ãƒŠãƒªã‚ªãƒ‡ãƒ¢"""
    print_section("8. å®Ÿä¸–ç•Œã‚·ãƒŠãƒªã‚ª", "ğŸŒ")
    
    scenarios = [
        {
            "name": "æœ¬ç•ªç’°å¢ƒéšœå®³",
            "description": "API endpoints returning 500 errors after deployment",
            "metrics": {"error_rate": 45, "response_time": "8.2s", "cpu": 98},
            "context": {"deployment_time": "2 hours ago", "affected_users": "~30%"}
        },
        {
            "name": "ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åŠ£åŒ–",
            "description": "Database queries becoming progressively slower",
            "metrics": {"query_time": "15s", "connections": 95, "cpu": 85},
            "context": {"trend": "increasing", "duration": "4 hours"}
        },
        {
            "name": "ãƒªã‚½ãƒ¼ã‚¹æ¯æ¸‡",
            "description": "Memory usage reaching critical levels",
            "metrics": {"memory": 96, "swap": 78, "gc_time": "2.1s"},
            "context": {"service": "user-service", "pods": 8}
        }
    ]
    
    for i, scenario in enumerate(scenarios, 1):
        print(f"\nğŸ¯ ã‚·ãƒŠãƒªã‚ª {i}: {scenario['name']}")
        print_success(f"èª¬æ˜: {scenario['description']}")
        print_success(f"ãƒ¡ãƒˆãƒªã‚¯ã‚¹: {len(scenario['metrics'])}é …ç›®")
        print_success(f"ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ: {len(scenario['context'])}é …ç›®")
        
        # å„ã‚·ãƒŠãƒªã‚ªã§ãƒ¢ãƒƒã‚¯åˆ†æã‚’å®Ÿè¡Œ
        request = LLMRequest(
            prompt=scenario['description'],
            template=PromptTemplate.INCIDENT_ANALYSIS,
            context={**scenario['metrics'], **scenario['context']}
        )
        
        print_success("âœ… åˆ†æãƒªã‚¯ã‚¨ã‚¹ãƒˆç”Ÿæˆå®Œäº†")


async def main():
    """ãƒ¡ã‚¤ãƒ³ãƒ‡ãƒ¢å®Ÿè¡Œ"""
    print("ğŸ§  AALS Module 6: LLM Wrapper ãƒ‡ãƒ¢")
    print("=" * 50)
    
    try:
        # 1. åˆæœŸåŒ–
        llm_wrapper = await demo_llm_wrapper_initialization()
        
        # 2. ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ç¢ºèª
        setup_ok = await demo_setup_verification(llm_wrapper)
        
        # 3. ãƒ¢ãƒƒã‚¯ãƒ¬ã‚¹ãƒãƒ³ã‚¹ç”Ÿæˆ
        await demo_mock_llm_response(llm_wrapper)
        
        # 4. ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ©Ÿèƒ½
        await demo_cache_functionality(llm_wrapper)
        
        # 5. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
        await demo_prompt_templates(llm_wrapper)
        
        # 6. ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
        await demo_error_handling()
        
        # 7. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆ
        await demo_performance_stats(llm_wrapper)
        
        # 8. å®Ÿä¸–ç•Œã‚·ãƒŠãƒªã‚ª
        await demo_real_world_scenarios(llm_wrapper)
        
        # æœ€çµ‚çµ±è¨ˆ
        print_section("9. æœ€çµ‚çµ±è¨ˆ", "ğŸ“Š")
        final_stats = llm_wrapper.get_stats()
        print_success(f"ãƒ‡ãƒ¢å®Œäº†!")
        print_success(f"ç·å®Ÿè¡Œæ©Ÿèƒ½: 8é …ç›®")
        print_success(f"åˆ†æã‚·ãƒŠãƒªã‚ª: 3ä»¶")
        print_success(f"ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ: {len(llm_wrapper.system_prompts)}ç¨®é¡")
        
        logger.info("LLM Wrapper demo completed",
                   features_tested=8,
                   scenarios_analyzed=3,
                   templates_available=len(llm_wrapper.system_prompts))
        
        print_section("ğŸ‰ Module 6: LLM Wrapper ãƒ‡ãƒ¢å®Œäº†!", "")
        print("   ğŸ§  AIæ¨è«–æ©Ÿèƒ½: å®Œå…¨å®Ÿè£…")
        print("   ğŸ”— Claude APIçµ±åˆ: å®Œäº†")
        print("   ğŸ“ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ: 3ç¨®é¡")
        print("   ğŸ’¾ ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚­ãƒ£ãƒƒã‚·ãƒ¥: æœ‰åŠ¹")
        print("   ğŸ“Š çµ±è¨ˆãƒ»åˆ†ææ©Ÿèƒ½: å®Ÿè£…æ¸ˆã¿")
        print("=" * 50)
        
    except Exception as e:
        print_error(f"ãƒ‡ãƒ¢å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {str(e)}")
        logger.error("LLM Wrapper demo failed", error=str(e), exception_type=type(e).__name__)
        return 1
    
    return 0


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)